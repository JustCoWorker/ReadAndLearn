
```sh
### 1. 将数据集上传到hadoop文件系统
wget http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz
tar -xzvf 20news-bydate.tar.gz
hadoop fs -mkdir -p /20news
hadoop fs -put 20news-bydate-train /20news
# 注意可能会报这个错误
# java.lang.InterruptedException
# 	at java.lang.Object.wait(Native Method)
# 	at java.lang.Thread.join(Thread.java:1252)
# 	at java.lang.Thread.join(Thread.java:1326)
# 	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:973)
# 	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:624)
# 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:801
# 但是此错误可以忽略（有博主说这个错误是hadoop的一个bug），检查一下HDFS，发现文件上上传成功的。
# http://172.20.62.34:50070/explorer.html#/20news/20news-bydate-train
### 2. Convert the full 20 newsgroups dataset into a< Text, Text > SequenceFile.
# 将文件转成单行文本
mahout seqdirectory -i /20news/20news-bydate-train/ -o 20news-sql -ow
# http://172.20.62.34:50070/explorer.html#/user/root/20news-sql
### 3. Convert and preprocesses the dataset into a < Text,VectorWritable > SequenceFile containing term frequencies for each document.
# 对样本进行向量化处理
mahout seq2sparse -i 20news-sql -o 20news-vectors-test -lnorm -nv -wt tfidf
### 4. Train the classifier
mahout trainnb -i 20news-vectors-test/tfidf-vectors -o nbmodel -li labelindex -ow -c
### 5. Test the classifier
mahout testnb -i 20news-vectors-test/tfidf-vectors -m nbmodel -l labelindex -ow -o 20news-testing -c
### 6 控制台会打印出结果
# 但是我的正确率竟然只有5%。。。肯定是有问题了，这个还需要再分析，但是基本的过程就是这样
### 7. 导出数据
bin/mahout seqdumper -i /user/root/20news-vectors-test/tfidf-vectors/part-r-00000 -o ./20news_testing.res
```

## 参考

1. https://blog.csdn.net/Ichimaru_Gin_/article/details/79133935
2. https://blog.csdn.net/hechenghai/article/details/50351718